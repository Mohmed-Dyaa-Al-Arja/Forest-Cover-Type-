{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b371729a",
   "metadata": {},
   "source": [
    "# 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('covtype.csv', low_memory=True)\n",
    "df.info(memory_usage='deep')\n",
    "target_col = \"Cover_Type\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081363bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Initial Data Overview:\")\n",
    "display(df.head())\n",
    "\n",
    "display(\"\\nMissing Values per Column:\")\n",
    "display(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a08fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"StudentID\", \"Name\"]\n",
    "cols_exist = [col for col in columns_to_remove if col in df.columns]\n",
    "\n",
    "if cols_exist:\n",
    "    print(f\"[INFO] Dropping columns: {cols_exist}\")\n",
    "    df = df.drop(columns=cols_exist)\n",
    "else:\n",
    "    print(\"[INFO] No columns to drop found in dataframe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ccd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def encode_dataframe(df: pd.DataFrame, target_col: str):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    if df_encoded[target_col].dtype == 'object' or df_encoded[target_col].dtype.name == 'category':\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[target_col] = le.fit_transform(df_encoded[target_col])\n",
    "        print(f\"[INFO] Target column '{target_col}' encoded: {list(le.classes_)} -> {list(range(len(le.classes_)))}\")\n",
    "\n",
    "    categorical_cols = df_encoded.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    if target_col in categorical_cols:\n",
    "        categorical_cols.remove(target_col)\n",
    "\n",
    "    if categorical_cols:\n",
    "        print(f\"[INFO] Encoding categorical columns: {categorical_cols}\")\n",
    "        df_encoded = pd.get_dummies(df_encoded, columns=categorical_cols, drop_first=True)\n",
    "    else:\n",
    "        print(\"[INFO] No categorical columns to encode.\")\n",
    "\n",
    "    return df_encoded\n",
    "df = encode_dataframe(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebb45a",
   "metadata": {},
   "source": [
    "# 2. Visualization Before Filling Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305bd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if not missing_counts.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_counts.plot(kind='bar', color='orange')\n",
    "    plt.title(\"Missing Values per Column\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] No missing values found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555be94f",
   "metadata": {},
   "source": [
    "# 3. Handle Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "num_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "    if df[col].isnull().any():\n",
    "        mode_val = df[col].mode()[0]\n",
    "        df[col].fillna(mode_val, inplace=True)\n",
    "        print(f\"Column {col} filled with Mode: {mode_val}\")\n",
    "\n",
    "for col in num_cols:\n",
    "    if df[col].isnull().any():\n",
    "        mean_val = df[col].mean()\n",
    "        df[col].fillna(mean_val, inplace=True)\n",
    "        print(f\"Column {col} filled with Mean: {mean_val:.2f}\")\n",
    "\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if not missing_counts.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_counts.plot(kind='bar', color='green')\n",
    "    plt.title(\"Missing Values After Filling\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] No missing values remaining.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f24378",
   "metadata": {},
   "source": [
    "# 4. Handle Duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    print(\"Examples of duplicate rows:\")\n",
    "    display(df[df.duplicated()].head())\n",
    "    \n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Duplicates removed\")\n",
    "else:\n",
    "    print(\"[INFO] No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea88fa",
   "metadata": {},
   "source": [
    "# 1. Basic Distributions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411da7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def plot_distributions_auto(df, sample_size=5000, max_unique_for_countplot=15, top_n_bar=10):\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "    cols = df_sample.columns.tolist()\n",
    "    \n",
    "    n_cols = 3\n",
    "    n_rows = math.ceil(len(cols) / n_cols)\n",
    "\n",
    "    row_heights = []\n",
    "    for idx in range(n_rows):\n",
    "        row_cols = cols[idx*n_cols:(idx+1)*n_cols]\n",
    "        max_height = 4\n",
    "        for col in row_cols:\n",
    "            n_unique = df_sample[col].nunique()\n",
    "            dtype = df_sample[col].dtype\n",
    "            if dtype in [\"int64\", \"float64\"]:\n",
    "                if n_unique > 15:\n",
    "                    max_height = max(max_height, 4)\n",
    "                elif 3 < n_unique <= max_unique_for_countplot:\n",
    "                    max_height = max(max_height, 3)\n",
    "                else:\n",
    "                    max_height = max(max_height, 3.5)\n",
    "            else:\n",
    "                if n_unique == 2:\n",
    "                    max_height = max(max_height, 4)\n",
    "                elif n_unique <= max_unique_for_countplot:\n",
    "                    max_height = max(max_height, 4.5)\n",
    "                else:\n",
    "                    max_height = max(max_height, 5)\n",
    "        row_heights.append(max_height)\n",
    "\n",
    "    total_height = sum(row_heights) + 1\n",
    "    plt.figure(figsize=(n_cols*6, total_height))\n",
    "\n",
    "    # Plot each column\n",
    "    for idx, col in enumerate(cols):\n",
    "        plt.subplot(n_rows, n_cols, idx + 1)\n",
    "        n_unique = df_sample[col].nunique()\n",
    "        dtype = df_sample[col].dtype\n",
    "\n",
    "        # Automatic plot selection\n",
    "        if dtype in [\"int64\", \"float64\"]:\n",
    "            if n_unique > 15:\n",
    "                sns.histplot(df_sample[col], kde=True, bins=30)\n",
    "                plt.title(f\"{col} Histogram & KDE\")\n",
    "            elif 3 < n_unique <= max_unique_for_countplot:\n",
    "                sns.boxplot(x=df_sample[col])\n",
    "                plt.title(f\"{col} Boxplot\")\n",
    "            else:\n",
    "                sns.violinplot(y=df_sample[col])\n",
    "                plt.title(f\"{col} Violin\")\n",
    "        else:\n",
    "            if n_unique == 2:\n",
    "                df_sample[col].value_counts().plot.pie(autopct=\"%1.1f%%\", startangle=90)\n",
    "                plt.title(f\"{col} Pie\")\n",
    "                plt.ylabel(\"\")\n",
    "            elif n_unique <= max_unique_for_countplot:\n",
    "                sns.countplot(x=df_sample[col], order=df_sample[col].value_counts().index)\n",
    "                plt.title(f\"{col} Countplot\")\n",
    "                plt.xticks(rotation=45)\n",
    "            else:\n",
    "                top_vals = df_sample[col].value_counts().nlargest(top_n_bar)\n",
    "                sns.barplot(x=top_vals.index, y=top_vals.values)\n",
    "                plt.title(f\"{col} Top {top_n_bar}\")\n",
    "                plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_distributions_auto(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d40c53",
   "metadata": {},
   "source": [
    "# 2. Compare features with Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99735398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "def compare_with_target(df, target_col):\n",
    "    features = [col for col in df.columns if col != target_col]\n",
    "    n = len(features)\n",
    "    n_cols = 3 \n",
    "    n_rows = math.ceil(n / n_cols)\n",
    "    \n",
    "    plt.figure(figsize=(18, n_rows * 4))\n",
    "    \n",
    "    for i, col in enumerate(features):\n",
    "        plt.subplot(n_rows, n_cols, i + 1)\n",
    "        \n",
    "        # Numeric feature vs Categorical target\n",
    "        if df[col].dtype in [\"int64\", \"float64\"] and df[target_col].dtype == \"object\":\n",
    "            if df[target_col].nunique() <= 10:\n",
    "                sns.boxplot(x=df[target_col], y=df[col])\n",
    "                plt.title(f\"{col} vs {target_col}\")\n",
    "            else:\n",
    "                sns.violinplot(x=df[target_col], y=df[col])\n",
    "                plt.title(f\"{col} vs {target_col}\")\n",
    "        \n",
    "        # Categorical feature vs Categorical target\n",
    "        elif df[col].dtype == \"object\" and df[target_col].dtype == \"object\":\n",
    "            if df[col].nunique() <= 10 and df[target_col].nunique() <= 10:\n",
    "                sns.countplot(x=df[col], hue=df[target_col])\n",
    "                plt.title(f\"{col} vs {target_col}\")\n",
    "            else:\n",
    "                top_vals = df[col].value_counts().nlargest(10).index\n",
    "                sns.countplot(x=df[col][df[col].isin(top_vals)], hue=df[target_col])\n",
    "                plt.title(f\"Top 10 {col} vs {target_col}\")\n",
    "                plt.xticks(rotation=45)\n",
    "        \n",
    "        # Numeric feature vs Numeric target\n",
    "        elif df[col].dtype in [\"int64\",\"float64\"] and df[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "            sns.scatterplot(x=df[col], y=df[target_col])\n",
    "            plt.title(f\"{col} vs {target_col}\")\n",
    "        \n",
    "        # Categorical feature vs Numeric target\n",
    "        elif df[col].dtype == \"object\" and df[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "            sns.boxplot(x=df[col], y=df[target_col])\n",
    "            plt.title(f\"{col} vs {target_col}\")\n",
    "            plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "compare_with_target(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372d09d",
   "metadata": {},
   "source": [
    "# 3. Compare features with each other (no target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be352c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def compare_features_auto(df, sample_size=5000, max_pairplots=500, max_categories=10, n_cols=3):\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\",\"category\"]).columns\n",
    "\n",
    "    df_sample = df.sample(min(sample_size, len(df)), random_state=42)\n",
    "\n",
    "    # 1. Correlation Heatmap\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12,10))\n",
    "        sns.heatmap(df_sample[numeric_cols].corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\",\n",
    "                    annot_kws={\"size\":8})\n",
    "        plt.title(\"Correlation Heatmap (Numeric Features)\", fontsize=16)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if len(numeric_cols) >= 2:\n",
    "        pairs = list(combinations(numeric_cols, 2))[:max_pairplots]\n",
    "        chunk_size = n_cols * 3  \n",
    "        for i in range(0, len(pairs), chunk_size):\n",
    "            chunk = pairs[i:i+chunk_size]\n",
    "            n_rows = int(np.ceil(len(chunk)/n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "            axes = np.array(axes).flatten()\n",
    "            for idx, (col1, col2) in enumerate(chunk):\n",
    "                sns.scatterplot(x=df_sample[col1], y=df_sample[col2], ax=axes[idx])\n",
    "                axes[idx].set_title(f\"{col1} vs {col2}\", fontsize=10)\n",
    "            for j in range(len(chunk), len(axes)):\n",
    "                axes[j].set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    if len(categorical_cols) >= 2:\n",
    "        cat_pairs = list(combinations(categorical_cols, 2))\n",
    "        chunk_size = n_cols * 3\n",
    "        for i in range(0, len(cat_pairs), chunk_size):\n",
    "            chunk = cat_pairs[i:i+chunk_size]\n",
    "            n_rows = int(np.ceil(len(chunk)/n_cols))\n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "            axes = np.array(axes).flatten()\n",
    "            for idx, (col1, col2) in enumerate(chunk):\n",
    "                top1 = df_sample[col1].value_counts().nlargest(max_categories).index\n",
    "                top2 = df_sample[col2].value_counts().nlargest(max_categories).index\n",
    "                crosstab = pd.crosstab(df_sample[col1].where(df_sample[col1].isin(top1)),\n",
    "                                       df_sample[col2].where(df_sample[col2].isin(top2)))\n",
    "                if crosstab.empty:\n",
    "                    axes[idx].set_visible(False)\n",
    "                    continue\n",
    "                sns.heatmap(crosstab, annot=False, cmap=\"Blues\", ax=axes[idx])\n",
    "                axes[idx].set_title(f\"{col1} vs {col2}\", fontsize=10)\n",
    "                axes[idx].tick_params(axis='x', rotation=45)\n",
    "                axes[idx].tick_params(axis='y', rotation=0)\n",
    "            for j in range(len(chunk), len(axes)):\n",
    "                axes[j].set_visible(False)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Usage\n",
    "compare_features_auto(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c826f95",
   "metadata": {},
   "source": [
    "# 4. Features vs Target + Others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f975fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def scatter_features_with_target_dynamic(df, target_col, sample_size=5000, n_cols=3):\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=[\"int64\",\"float64\"]).columns if col != target_col]\n",
    "    if not numeric_cols:\n",
    "        print(\"No numeric features found.\")\n",
    "        return\n",
    "\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "\n",
    "    n_rows = math.ceil(len(numeric_cols) / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*5, n_rows*4))\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for idx, col in enumerate(numeric_cols):\n",
    "        ax = axes[idx]\n",
    "        if df_sample[target_col].dtype in [\"int64\",\"float64\"]:\n",
    "            sns.regplot(x=df_sample[col], y=df_sample[target_col], scatter_kws={\"alpha\":0.5}, ax=ax)\n",
    "            ax.set_title(f\"{col} vs {target_col} Scatter + Reg\")\n",
    "        elif df_sample[target_col].dtype == \"object\":\n",
    "            sns.boxplot(x=df_sample[target_col], y=df_sample[col], ax=ax)\n",
    "            sns.stripplot(x=df_sample[target_col], y=df_sample[col], color=\"black\", alpha=0.3, ax=ax)\n",
    "            ax.set_title(f\"{col} vs {target_col} Box + Strip\")\n",
    "\n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "scatter_features_with_target_dynamic(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e63dc0",
   "metadata": {},
   "source": [
    "# 5. Compare feature with itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a783509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "def self_comparison_dynamic(df, sample_size=5000, n_cols=3):\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "    if numeric_cols.empty:\n",
    "        print(\"No numeric features found.\")\n",
    "        return\n",
    "\n",
    "    # Sampling\n",
    "    df_sample = df.sample(sample_size, random_state=42) if len(df) > sample_size else df\n",
    "\n",
    "    plot_types = [\"hist_kde\", \"box\", \"violin\", \"ecdf\", \"qq\"]\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        n_plots = len(plot_types)\n",
    "        n_rows = math.ceil(n_plots / n_cols)\n",
    "        fig, axes = plt.subplots(n_rows, min(n_cols, n_plots), figsize=(n_cols*5, n_rows*4))\n",
    "        axes = np.array(axes).flatten()\n",
    "\n",
    "        for idx, plot_type in enumerate(plot_types):\n",
    "            ax = axes[idx]\n",
    "            if plot_type == \"hist_kde\":\n",
    "                sns.histplot(df_sample[col], kde=True, bins=30, color=\"steelblue\", ax=ax)\n",
    "                ax.set_title(f\"Histogram & KDE of {col}\")\n",
    "            elif plot_type == \"box\":\n",
    "                sns.boxplot(x=df_sample[col], color=\"orange\", ax=ax)\n",
    "                ax.set_title(f\"Boxplot of {col}\")\n",
    "            elif plot_type == \"violin\":\n",
    "                sns.violinplot(y=df_sample[col], color=\"purple\", ax=ax)\n",
    "                ax.set_title(f\"Violin Plot of {col}\")\n",
    "            elif plot_type == \"ecdf\":\n",
    "                sns.ecdfplot(df_sample[col], color=\"green\", ax=ax)\n",
    "                ax.set_title(f\"ECDF of {col}\")\n",
    "            elif plot_type == \"qq\":\n",
    "                stats.probplot(df_sample[col], dist=\"norm\", plot=ax)\n",
    "                ax.set_title(f\"Q-Q Plot of {col}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Usage\n",
    "self_comparison_dynamic(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d879663",
   "metadata": {},
   "source": [
    "# 6. Extra Ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f147541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, prange\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "    _IMBLEARN_OK = True\n",
    "except:\n",
    "    _IMBLEARN_OK = False\n",
    "\n",
    "# ====================== Fast LOF with Numba ======================\n",
    "@njit(parallel=True)\n",
    "def pairwise_distance(X):\n",
    "    n_samples = X.shape[0]\n",
    "    distances = np.empty((n_samples, n_samples), dtype=np.float32)\n",
    "    for i in prange(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            diff = X[i] - X[j]\n",
    "            distances[i, j] = np.sqrt(np.dot(diff, diff))\n",
    "    return distances\n",
    "\n",
    "@njit\n",
    "def local_reachability_density(distances, k):\n",
    "    n_samples = distances.shape[0]\n",
    "    lrd = np.zeros(n_samples, dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        sorted_idx = np.argsort(distances[i])\n",
    "        neighbors_idx = sorted_idx[1:k+1]\n",
    "        reach_dist_sum = 0.0\n",
    "        for j in neighbors_idx:\n",
    "            k_dist_j = distances[j][np.argsort(distances[j])[k]]\n",
    "            reach_dist = max(k_dist_j, distances[i, j])\n",
    "            reach_dist_sum += reach_dist\n",
    "        lrd[i] = k / reach_dist_sum if reach_dist_sum > 0 else 0\n",
    "    return lrd\n",
    "\n",
    "@njit\n",
    "def lof_score(lrd, distances, k):\n",
    "    n_samples = distances.shape[0]\n",
    "    lof = np.zeros(n_samples, dtype=np.float32)\n",
    "    for i in range(n_samples):\n",
    "        sorted_idx = np.argsort(distances[i])\n",
    "        neighbors_idx = sorted_idx[1:k+1]\n",
    "        sum_ratio = 0.0\n",
    "        for j in neighbors_idx:\n",
    "            if lrd[i] > 0:\n",
    "                sum_ratio += lrd[j] / lrd[i]\n",
    "        lof[i] = sum_ratio / k\n",
    "    return lof\n",
    "\n",
    "def smart_lof_mask(df, numeric_cols, k=20, threshold=1.5):\n",
    "    X = df[numeric_cols].fillna(df[numeric_cols].median()).values.astype(np.float32)\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    distances = pairwise_distance(X)\n",
    "    lrd = local_reachability_density(distances, k)\n",
    "    lof = lof_score(lrd, distances, k)\n",
    "    mask = lof <= threshold\n",
    "    print(f\"[INFO] LOF: removed {(~mask).sum()} rows ({100*(~mask).sum()/len(df):.2f}%)\")\n",
    "    return mask\n",
    "\n",
    "# ====================== Smart Preprocessing + Balancing ======================\n",
    "def smart_preprocess_balance_fast(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    drop_cols: list = None,\n",
    "    outlier_contamination: float = 0.01,\n",
    "    balance_method: str = \"smote\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    df2 = df.copy()\n",
    "    if drop_cols:\n",
    "        df2.drop(columns=[c for c in drop_cols if c in df2.columns], inplace=True)\n",
    "\n",
    "    if target_col not in df2.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found.\")\n",
    "\n",
    "    cat_cols = df2.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    if target_col in cat_cols:\n",
    "        cat_cols.remove(target_col)\n",
    "    num_cols = df2.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "\n",
    "    print(f\"[INFO] Categorical cols: {cat_cols}\")\n",
    "    print(f\"[INFO] Numerical cols: {num_cols}\")\n",
    "\n",
    "    # ---- Outlier Removal ----\n",
    "    if len(num_cols) > 0:\n",
    "        n_rows, n_cols_df = df2.shape\n",
    "        method_used = None\n",
    "        if n_rows > 10000:  \n",
    "            print(\"[INFO] Using IsolationForest for outlier removal\")\n",
    "            iso = IsolationForest(contamination=outlier_contamination, random_state=random_state)\n",
    "            mask = iso.fit_predict(df2[num_cols].fillna(df2[num_cols].median())) != -1\n",
    "            method_used = \"IsolationForest\"\n",
    "        elif n_cols_df <= 5:  \n",
    "            print(\"[INFO] Using EllipticEnvelope for outlier removal\")\n",
    "            env = EllipticEnvelope(contamination=outlier_contamination, random_state=random_state)\n",
    "            mask = env.fit_predict(df2[num_cols].fillna(df2[num_cols].median())) != -1\n",
    "            method_used = \"EllipticEnvelope\"\n",
    "        else:\n",
    "            print(\"[INFO] Using fast LOF for outlier removal\")\n",
    "            mask = smart_lof_mask(df2, num_cols)\n",
    "            method_used = \"LOF\"\n",
    "\n",
    "        df2 = df2.loc[mask].reset_index(drop=True)\n",
    "        print(f\"[INFO] Outlier removal done using {method_used}, new shape: {df2.shape}\")\n",
    "\n",
    "    # ---- Preprocessing Pipelines ----\n",
    "    num_transformer = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_transformer = None\n",
    "    if len(cat_cols) > 0:\n",
    "        cat_transformer = Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))\n",
    "        ])\n",
    "\n",
    "    transformers = [(\"num\", num_transformer, num_cols)]\n",
    "    if cat_transformer:\n",
    "        transformers.append((\"cat\", cat_transformer, cat_cols))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
    "\n",
    "    X = df2.drop(columns=[target_col])\n",
    "    y = df2[target_col]\n",
    "\n",
    "    # ---- Detect Task Type ----\n",
    "    if (pd.api.types.is_integer_dtype(y) or pd.api.types.is_object_dtype(y)) and y.nunique() < 20:\n",
    "        task_type = \"classification\"\n",
    "        if pd.api.types.is_object_dtype(y):\n",
    "            y = y.astype('category').cat.codes\n",
    "    else:\n",
    "        task_type = \"regression\"\n",
    "    print(f\"[INFO] Detected task type: {task_type}\")\n",
    "\n",
    "    # ---- Split ----\n",
    "    if task_type == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "        print(\"[INFO] Stratified split applied.\")\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=random_state)\n",
    "        print(\"[INFO] Normal split applied.\")\n",
    "\n",
    "    # ---- Preprocess ----\n",
    "    X_train_proc = preprocessor.fit_transform(X_train)\n",
    "    X_test_proc = preprocessor.transform(X_test)\n",
    "\n",
    "    # ---- Feature Names ----\n",
    "    feat_names = None\n",
    "    try:\n",
    "        cat_names = []\n",
    "        if cat_transformer:\n",
    "            cat_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols)\n",
    "        feat_names = np.concatenate([num_cols, cat_names])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ---- Balancing ----\n",
    "    if task_type == \"classification\" and _IMBLEARN_OK and balance_method.lower() != \"none\":\n",
    "        sampler = None\n",
    "        if balance_method.lower() == \"smote\":\n",
    "            if len(cat_cols) > 0:\n",
    "                try:\n",
    "                    cat_idx = list(range(len(num_cols), X_train_proc.shape[1]))\n",
    "                    sampler = SMOTENC(categorical_features=cat_idx, random_state=random_state, n_jobs=-1)\n",
    "                except Exception:\n",
    "                    sampler = SMOTE(random_state=random_state, n_jobs=-1)\n",
    "            else:\n",
    "                sampler = SMOTE(random_state=random_state, n_jobs=-1)\n",
    "        elif balance_method.lower() in [\"ros\", \"randomoversampler\"]:\n",
    "            sampler = RandomOverSampler(random_state=random_state)\n",
    "\n",
    "        if sampler:\n",
    "            X_train_proc, y_train = sampler.fit_resample(X_train_proc, y_train)\n",
    "            print(f\"[INFO] Balanced training set: {len(y_train)} samples\")\n",
    "\n",
    "    return X_train_proc, X_test_proc, y_train, y_test, preprocessor, feat_names\n",
    "\n",
    "X_train_proc, X_test_proc, y_train, y_test, preprocessor, feat_names = smart_preprocess_balance_fast(\n",
    "    df=df,\n",
    "    target_col=target_col,\n",
    "    outlier_contamination=0.01,\n",
    "    balance_method=\"smote\",\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "if feat_names is not None:\n",
    "    X_train_df = pd.DataFrame(\n",
    "        X_train_proc.toarray() if hasattr(X_train_proc, 'toarray') else X_train_proc,\n",
    "        columns=feat_names\n",
    "    )\n",
    "    print(X_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2650f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_squared_error\n",
    "\n",
    "# Classic models\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor, BayesianRidge, PassiveAggressiveClassifier, PassiveAggressiveRegressor, Perceptron, LinearRegression, HuberRegressor, Lars, LassoLars, OrthogonalMatchingPursuit, ARDRegression, TweedieRegressor, PoissonRegressor, GammaRegressor, QuantileRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, BaggingClassifier, BaggingRegressor, VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier, RadiusNeighborsRegressor, NearestCentroid\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Optional models\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "    _LGBM_OK = True\n",
    "except: _LGBM_OK = False\n",
    "try:\n",
    "    from xgboost import XGBClassifier, XGBRegressor\n",
    "    _XGB_OK = True\n",
    "except: _XGB_OK = False\n",
    "try:\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "    _CAT_OK = True\n",
    "except: _CAT_OK = False\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    _PROPHET_OK = True\n",
    "except: _PROPHET_OK = False\n",
    "try:\n",
    "    from pmdarima import auto_arima\n",
    "    _PMDARIMA_OK = True\n",
    "except: _PMDARIMA_OK = False\n",
    "\n",
    "#  Numba for fast sorting (used in feature importance, etc) \n",
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def numba_argsort(arr):\n",
    "    return np.argsort(arr)\n",
    "\n",
    "def print_step(msg):\n",
    "    print(f\"\\n{'='*10} {msg} {'='*10}\")\n",
    "\n",
    "def detect_problem_type(df, target_col, time_col=None):\n",
    "    if time_col and time_col in df.columns:\n",
    "        try:\n",
    "            pd.to_datetime(df[time_col])\n",
    "            return \"time_series\"\n",
    "        except: pass\n",
    "    y = df[target_col]\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        if (y.dropna() % 1 == 0).all() and y.nunique(dropna=True) <= 20:\n",
    "            return \"classification\"\n",
    "        else:\n",
    "            return \"regression\"\n",
    "    return \"classification\"\n",
    "\n",
    "def preprocess_data(df, target_col, time_col=None):\n",
    "    df = df.copy()\n",
    "    if time_col and time_col in df.columns:\n",
    "        df = df.sort_values(time_col)\n",
    "    cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "    if target_col in num_cols: num_cols.remove(target_col)\n",
    "    num_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "    ]) if cat_cols else \"drop\"\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", num_pipe, num_cols),\n",
    "        (\"cat\", cat_pipe, cat_cols)\n",
    "    ])\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_proc = preprocessor.fit_transform(X)\n",
    "    feat_names = []\n",
    "    feat_names.extend(num_cols)\n",
    "    if cat_cols:\n",
    "        feat_names.extend(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols))\n",
    "    return X_proc, y, feat_names\n",
    "\n",
    "def get_models(task, n_samples=None):\n",
    "    # Reduce max_iter for heavy models if data is large\n",
    "    heavy_iter = 200 if n_samples is not None and n_samples > 50000 else 2000\n",
    "    mlp_iter = 100 if n_samples is not None and n_samples > 50000 else 300\n",
    "    lgbm_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "    xgb_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "    cat_estimators = 50 if n_samples is not None and n_samples > 50000 else 200\n",
    "\n",
    "    models = []\n",
    "    if task == \"classification\":\n",
    "        models += [\n",
    "            (\"RandomForest\", RandomForestClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"HistGB\", HistGradientBoostingClassifier(max_iter=lgbm_estimators, random_state=42)),\n",
    "            (\"DecisionTree\", DecisionTreeClassifier(random_state=42)),\n",
    "            (\"ExtraTree\", ExtraTreeClassifier(random_state=42)),\n",
    "            (\"AdaBoost\", AdaBoostClassifier(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"Bagging\", BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "            (\"LogisticRegression\", LogisticRegression(max_iter=heavy_iter, n_jobs=-1, random_state=42)),\n",
    "            (\"SGDClassifier\", SGDClassifier(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"LinearSVC\", LinearSVC(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"RidgeClassifier\", Ridge()),\n",
    "            (\"PassiveAggressiveClassifier\", PassiveAggressiveClassifier(max_iter=1000, random_state=42)),\n",
    "            (\"Perceptron\", Perceptron(max_iter=1000, random_state=42)),\n",
    "            (\"GaussianNB\", GaussianNB()),\n",
    "            (\"BernoulliNB\", BernoulliNB()),\n",
    "            (\"MultinomialNB\", MultinomialNB()),\n",
    "            (\"ComplementNB\", ComplementNB()),\n",
    "            (\"LDA\", LinearDiscriminantAnalysis()),\n",
    "            (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "            (\"KNN\", KNeighborsClassifier(n_jobs=-1)),\n",
    "            (\"RadiusNN\", RadiusNeighborsClassifier(n_jobs=-1, outlier_label=0)),\n",
    "            (\"NearestCentroid\", NearestCentroid()),\n",
    "            (\"MLP\", MLPClassifier(hidden_layer_sizes=(64,32), max_iter=mlp_iter, random_state=42)),\n",
    "        ]\n",
    "        if _LGBM_OK: models.append((\"LightGBM\", LGBMClassifier(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)))\n",
    "        if _XGB_OK:  models.append((\"XGBoost\", XGBClassifier(n_estimators=xgb_estimators, n_jobs=-1, random_state=42, verbosity=0, use_label_encoder=False)))\n",
    "        if _CAT_OK:  models.append((\"CatBoost\", CatBoostClassifier(n_estimators=cat_estimators, verbose=0, random_state=42)))\n",
    "    else:\n",
    "        models += [\n",
    "            (\"RandomForest\", RandomForestRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"ExtraTrees\", ExtraTreesRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)),\n",
    "            (\"GradientBoosting\", GradientBoostingRegressor(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"HistGB\", HistGradientBoostingRegressor(max_iter=lgbm_estimators, random_state=42)),\n",
    "            (\"DecisionTree\", DecisionTreeRegressor(random_state=42)),\n",
    "            (\"ExtraTree\", ExtraTreeRegressor(random_state=42)),\n",
    "            (\"AdaBoost\", AdaBoostRegressor(n_estimators=lgbm_estimators, random_state=42)),\n",
    "            (\"Bagging\", BaggingRegressor(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "            (\"LinearRegression\", LinearRegression()),\n",
    "            (\"Ridge\", Ridge()),\n",
    "            (\"Lasso\", Lasso()),\n",
    "            (\"ElasticNet\", ElasticNet()),\n",
    "            (\"SGDRegressor\", SGDRegressor(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"BayesianRidge\", BayesianRidge()),\n",
    "            (\"HuberRegressor\", HuberRegressor()),\n",
    "            (\"Lars\", Lars()),\n",
    "            (\"LassoLars\", LassoLars()),\n",
    "            (\"OrthogonalMatchingPursuit\", OrthogonalMatchingPursuit()),\n",
    "            (\"ARDRegression\", ARDRegression()),\n",
    "            (\"TweedieRegressor\", TweedieRegressor()),\n",
    "            (\"PoissonRegressor\", PoissonRegressor()),\n",
    "            (\"GammaRegressor\", GammaRegressor()),\n",
    "            (\"QuantileRegressor\", QuantileRegressor()),\n",
    "            (\"PassiveAggressiveRegressor\", PassiveAggressiveRegressor(max_iter=1000, random_state=42)),\n",
    "            (\"KNN\", KNeighborsRegressor(n_jobs=-1)),\n",
    "            (\"RadiusNN\", RadiusNeighborsRegressor(n_jobs=-1)),\n",
    "            (\"LinearSVR\", LinearSVR(max_iter=heavy_iter, random_state=42)),\n",
    "            (\"MLP\", MLPRegressor(hidden_layer_sizes=(64,32), max_iter=mlp_iter, random_state=42)),\n",
    "        ]\n",
    "        if _LGBM_OK: models.append((\"LightGBM\", LGBMRegressor(n_estimators=lgbm_estimators, n_jobs=-1, random_state=42)))\n",
    "        if _XGB_OK:  models.append((\"XGBoost\", XGBRegressor(n_estimators=xgb_estimators, n_jobs=-1, random_state=42, verbosity=0)))\n",
    "        if _CAT_OK:  models.append((\"CatBoost\", CatBoostRegressor(n_estimators=cat_estimators, verbose=0, random_state=42)))\n",
    "    return models\n",
    "\n",
    "def auto_ml_ultra(df, target_col, time_col=None, min_score=0.88):\n",
    "    import joblib\n",
    "    t0 = time.time()\n",
    "    print_step(\"Detecting problem type\")\n",
    "    task = detect_problem_type(df, target_col, time_col)\n",
    "    print(f\"[INFO] Task: {task}\")\n",
    "\n",
    "    if task == \"time_series\":\n",
    "        if _PROPHET_OK:\n",
    "            print_step(\"Prophet Forecasting\")\n",
    "            df2 = df[[time_col, target_col]].rename(columns={time_col: \"ds\", target_col: \"y\"})\n",
    "            m = Prophet()\n",
    "            m.fit(df2)\n",
    "            future = m.make_future_dataframe(periods=10)\n",
    "            forecast = m.predict(future)\n",
    "            print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "        if _PMDARIMA_OK:\n",
    "            print_step(\"ARIMA Forecasting\")\n",
    "            model = auto_arima(df[target_col], seasonal=False, trace=True)\n",
    "            print(model.summary())\n",
    "        print(\"[INFO] Time Series task finished.\")\n",
    "        return\n",
    "\n",
    "    print_step(\"Preprocessing\")\n",
    "    X_proc, y, feat_names = preprocess_data(df, target_col, time_col)\n",
    "    print(f\"[INFO] Features: {X_proc.shape[1]} | Samples: {X_proc.shape[0]}\")\n",
    "\n",
    "    print_step(\"Splitting data\")\n",
    "    if task == \"classification\":\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        score_metric = \"accuracy\"\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42)\n",
    "        cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        score_metric = \"r2\"\n",
    "    print(f\"[INFO] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    print_step(\"Model selection & training\")\n",
    "    models = get_models(task, n_samples=X_train.shape[0])\n",
    "    results = []\n",
    "    results_table = []\n",
    "    for idx, (name, model) in enumerate(models):\n",
    "        t1 = time.time()\n",
    "        try:\n",
    "            scores = cross_val_score(model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "            results.append((score, name, model))\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": time.time()-t1\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1:02d}/{len(models)}] {name}: ERROR {e}\")\n",
    "            results_table.append({\n",
    "                \"Model\": name,\n",
    "                \"CV_Mean\": np.nan,\n",
    "                \"CV_Std\": np.nan,\n",
    "                \"Time_sec\": time.time()-t1,\n",
    "                \"Error\": str(e)\n",
    "            })\n",
    "    results.sort(reverse=True)\n",
    "    top_models = results[:10]\n",
    "    best_score, best_name, best_model = top_models[0]\n",
    "\n",
    "    # Voting/Stacking\n",
    "    print_step(\"Ensemble (Voting/Stacking)\")\n",
    "    try:\n",
    "        if task == \"classification\":\n",
    "            voting = VotingClassifier(estimators=[(n, m) for _, n, m in top_models], voting='soft', n_jobs=-1)\n",
    "            stacking = StackingClassifier(estimators=[(n, m) for _, n, m in top_models], final_estimator=LogisticRegression(max_iter=2000), n_jobs=-1)\n",
    "        else:\n",
    "            voting = VotingRegressor(estimators=[(n, m) for _, n, m in top_models], n_jobs=-1)\n",
    "            stacking = StackingRegressor(estimators=[(n, m) for _, n, m in top_models], final_estimator=Ridge(), n_jobs=-1)\n",
    "        for ens_name, ens_model in [(\"Voting\", voting), (\"Stacking\", stacking)]:\n",
    "            t1 = time.time()\n",
    "            scores = cross_val_score(ens_model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "            score = scores.mean()\n",
    "            std = scores.std()\n",
    "            print(f\"[Ensemble] {ens_name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "            results.append((score, ens_name, ens_model))\n",
    "            results_table.append({\n",
    "                \"Model\": ens_name,\n",
    "                \"CV_Mean\": score,\n",
    "                \"CV_Std\": std,\n",
    "                \"Time_sec\": time.time()-t1\n",
    "            })\n",
    "            if score > best_score:\n",
    "                best_score, best_name, best_model = score, ens_name, ens_model\n",
    "    except Exception as e:\n",
    "        print(f\"Ensemble Error: {e}\")\n",
    "\n",
    "    print_step(f\"Best Model: {best_name} (CV {score_metric}: {best_score:.4f})\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    # Save best model\n",
    "    joblib.dump(best_model, \"best_model.pkl\")\n",
    "    print(\"[INFO] Best model saved as best_model.pkl\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    if task == \"classification\":\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Test Accuracy: {acc:.4f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(\"Confusion Matrix:\\n\", cm)\n",
    "        if acc < min_score:\n",
    "            print(f\"\\n[WARNING] Accuracy is less than {min_score*100:.1f}%!\")\n",
    "    else:\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(f\"Test R2: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "        if r2 < min_score:\n",
    "            print(f\"\\n[WARNING] R2 is less than {min_score*100:.1f}%!\")\n",
    "\n",
    "    if hasattr(best_model, \"feature_importances_\"):\n",
    "        importances = best_model.feature_importances_\n",
    "        idx = numba_argsort(importances)[::-1][:10]\n",
    "        print_step(\"Top 10 Features\")\n",
    "        for i in idx:\n",
    "            print(f\"{feat_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "    print(f\"\\n[INFO] Total pipeline time: {time.time()-t0:.1f} sec.\")\n",
    "\n",
    "    print_step(\"Summary Table (All Models)\")\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "    df_results = df_results.sort_values(\"CV_Mean\", ascending=False)\n",
    "    display(df_results.reset_index(drop=True))\n",
    "\n",
    "\n",
    "auto_ml_ultra(df, target_col, time_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbee9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "X = df.drop(target_col, axis=1)\n",
    "y = df[target_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Bagging: Test accuracy = {accuracy:.4f} | Time: {end_time - start_time:.1f}s\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "joblib.dump(model, \"bagging_model.pkl\")\n",
    "print(\"bagging_model.pkl saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import time\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.impute import SimpleImputer\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, r2_score, mean_squared_error\n",
    "\n",
    "# # Classic models\n",
    "# from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor, BayesianRidge, PassiveAggressiveClassifier, PassiveAggressiveRegressor, Perceptron, LinearRegression, HuberRegressor, Lars, LassoLars, OrthogonalMatchingPursuit, ARDRegression, TweedieRegressor, PoissonRegressor, GammaRegressor, QuantileRegressor\n",
    "# from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor\n",
    "# from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor, BaggingClassifier, BaggingRegressor, VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor, HistGradientBoostingClassifier, HistGradientBoostingRegressor\n",
    "# from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, RadiusNeighborsClassifier, RadiusNeighborsRegressor, NearestCentroid\n",
    "# from sklearn.svm import LinearSVC, LinearSVR\n",
    "# from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# # Optional models\n",
    "# try:\n",
    "#     from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "#     _LGBM_OK = True\n",
    "# except: _LGBM_OK = False\n",
    "# try:\n",
    "#     from xgboost import XGBClassifier, XGBRegressor\n",
    "#     _XGB_OK = True\n",
    "# except: _XGB_OK = False\n",
    "# try:\n",
    "#     from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "#     _CAT_OK = True\n",
    "# except: _CAT_OK = False\n",
    "# try:\n",
    "#     from prophet import Prophet\n",
    "#     _PROPHET_OK = True\n",
    "# except: _PROPHET_OK = False\n",
    "# try:\n",
    "#     from pmdarima import auto_arima\n",
    "#     _PMDARIMA_OK = True\n",
    "# except: _PMDARIMA_OK = False\n",
    "\n",
    "# def print_step(msg):\n",
    "#     print(f\"\\n{'='*10} {msg} {'='*10}\")\n",
    "\n",
    "# def detect_problem_type(df, target_col, time_col=None):\n",
    "#     if time_col and time_col in df.columns:\n",
    "#         try:\n",
    "#             pd.to_datetime(df[time_col])\n",
    "#             return \"time_series\"\n",
    "#         except: pass\n",
    "#     y = df[target_col]\n",
    "#     if pd.api.types.is_numeric_dtype(y):\n",
    "#         if (y.dropna() % 1 == 0).all() and y.nunique(dropna=True) <= 20:\n",
    "#             return \"classification\"\n",
    "#         else:\n",
    "#             return \"regression\"\n",
    "#     return \"classification\"\n",
    "\n",
    "# def preprocess_data(df, target_col, time_col=None):\n",
    "#     df = df.copy()\n",
    "#     if time_col and time_col in df.columns:\n",
    "#         df = df.sort_values(time_col)\n",
    "#     cat_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()\n",
    "#     num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "#     if target_col in cat_cols: cat_cols.remove(target_col)\n",
    "#     if target_col in num_cols: num_cols.remove(target_col)\n",
    "#     num_pipe = Pipeline([\n",
    "#         (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#         (\"scaler\", StandardScaler())\n",
    "#     ])\n",
    "#     cat_pipe = Pipeline([\n",
    "#         (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#         (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
    "#     ]) if cat_cols else \"drop\"\n",
    "#     preprocessor = ColumnTransformer([\n",
    "#         (\"num\", num_pipe, num_cols),\n",
    "#         (\"cat\", cat_pipe, cat_cols)\n",
    "#     ])\n",
    "#     X = df.drop(columns=[target_col])\n",
    "#     y = df[target_col]\n",
    "#     X_proc = preprocessor.fit_transform(X)\n",
    "#     feat_names = []\n",
    "#     feat_names.extend(num_cols)\n",
    "#     if cat_cols:\n",
    "#         feat_names.extend(preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(cat_cols))\n",
    "#     return X_proc, y, feat_names\n",
    "\n",
    "# def get_models(task):\n",
    "#     models = []\n",
    "#     if task == \"classification\":\n",
    "#         models += [\n",
    "#             (\"RandomForest\", RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)),\n",
    "#             (\"ExtraTrees\", ExtraTreesClassifier(n_estimators=200, n_jobs=-1, random_state=42)),\n",
    "#             (\"GradientBoosting\", GradientBoostingClassifier(n_estimators=200, random_state=42)),\n",
    "#             (\"HistGB\", HistGradientBoostingClassifier(max_iter=200, random_state=42)),\n",
    "#             (\"DecisionTree\", DecisionTreeClassifier(random_state=42)),\n",
    "#             (\"ExtraTree\", ExtraTreeClassifier(random_state=42)),\n",
    "#             (\"AdaBoost\", AdaBoostClassifier(n_estimators=200, random_state=42)),\n",
    "#             (\"Bagging\", BaggingClassifier(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "#             (\"LogisticRegression\", LogisticRegression(max_iter=2000, n_jobs=-1, random_state=42)),\n",
    "#             (\"SGDClassifier\", SGDClassifier(max_iter=2000, random_state=42)),\n",
    "#             (\"LinearSVC\", LinearSVC(max_iter=2000, random_state=42)),\n",
    "#             (\"RidgeClassifier\", Ridge()),\n",
    "#             (\"PassiveAggressiveClassifier\", PassiveAggressiveClassifier(max_iter=1000, random_state=42)),\n",
    "#             (\"Perceptron\", Perceptron(max_iter=1000, random_state=42)),\n",
    "#             (\"GaussianNB\", GaussianNB()),\n",
    "#             (\"BernoulliNB\", BernoulliNB()),\n",
    "#             (\"MultinomialNB\", MultinomialNB()),\n",
    "#             (\"ComplementNB\", ComplementNB()),\n",
    "#             (\"LDA\", LinearDiscriminantAnalysis()),\n",
    "#             (\"QDA\", QuadraticDiscriminantAnalysis()),\n",
    "#             (\"KNN\", KNeighborsClassifier(n_jobs=-1)),\n",
    "#             (\"RadiusNN\", RadiusNeighborsClassifier(n_jobs=-1, outlier_label=0)),\n",
    "#             (\"NearestCentroid\", NearestCentroid()),\n",
    "#             (\"MLP\", MLPClassifier(hidden_layer_sizes=(64,32), max_iter=300, random_state=42)),\n",
    "#         ]\n",
    "#         if _LGBM_OK: models.append((\"LightGBM\", LGBMClassifier(n_estimators=200, n_jobs=-1, random_state=42)))\n",
    "#         if _XGB_OK:  models.append((\"XGBoost\", XGBClassifier(n_estimators=200, n_jobs=-1, random_state=42, verbosity=0, use_label_encoder=False)))\n",
    "#         if _CAT_OK:  models.append((\"CatBoost\", CatBoostClassifier(n_estimators=200, verbose=0, random_state=42)))\n",
    "#     else:\n",
    "#         models += [\n",
    "#             (\"RandomForest\", RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)),\n",
    "#             (\"ExtraTrees\", ExtraTreesRegressor(n_estimators=200, n_jobs=-1, random_state=42)),\n",
    "#             (\"GradientBoosting\", GradientBoostingRegressor(n_estimators=200, random_state=42)),\n",
    "#             (\"HistGB\", HistGradientBoostingRegressor(max_iter=200, random_state=42)),\n",
    "#             (\"DecisionTree\", DecisionTreeRegressor(random_state=42)),\n",
    "#             (\"ExtraTree\", ExtraTreeRegressor(random_state=42)),\n",
    "#             (\"AdaBoost\", AdaBoostRegressor(n_estimators=200, random_state=42)),\n",
    "#             (\"Bagging\", BaggingRegressor(n_estimators=100, n_jobs=-1, random_state=42)),\n",
    "#             (\"LinearRegression\", LinearRegression()),\n",
    "#             (\"Ridge\", Ridge()),\n",
    "#             (\"Lasso\", Lasso()),\n",
    "#             (\"ElasticNet\", ElasticNet()),\n",
    "#             (\"SGDRegressor\", SGDRegressor(max_iter=2000, random_state=42)),\n",
    "#             (\"BayesianRidge\", BayesianRidge()),\n",
    "#             (\"HuberRegressor\", HuberRegressor()),\n",
    "#             (\"Lars\", Lars()),\n",
    "#             (\"LassoLars\", LassoLars()),\n",
    "#             (\"OrthogonalMatchingPursuit\", OrthogonalMatchingPursuit()),\n",
    "#             (\"ARDRegression\", ARDRegression()),\n",
    "#             (\"TweedieRegressor\", TweedieRegressor()),\n",
    "#             (\"PoissonRegressor\", PoissonRegressor()),\n",
    "#             (\"GammaRegressor\", GammaRegressor()),\n",
    "#             (\"QuantileRegressor\", QuantileRegressor()),\n",
    "#             (\"PassiveAggressiveRegressor\", PassiveAggressiveRegressor(max_iter=1000, random_state=42)),\n",
    "#             (\"KNN\", KNeighborsRegressor(n_jobs=-1)),\n",
    "#             (\"RadiusNN\", RadiusNeighborsRegressor(n_jobs=-1)),\n",
    "\n",
    "#             (\"LinearSVR\", LinearSVR(max_iter=2000, random_state=42)),\n",
    "#             (\"MLP\", MLPRegressor(hidden_layer_sizes=(64,32), max_iter=300, random_state=42)),\n",
    "#         ]\n",
    "#         if _LGBM_OK: models.append((\"LightGBM\", LGBMRegressor(n_estimators=200, n_jobs=-1, random_state=42)))\n",
    "#         if _XGB_OK:  models.append((\"XGBoost\", XGBRegressor(n_estimators=200, n_jobs=-1, random_state=42, verbosity=0)))\n",
    "#         if _CAT_OK:  models.append((\"CatBoost\", CatBoostRegressor(n_estimators=200, verbose=0, random_state=42)))\n",
    "#     return models\n",
    "\n",
    "# def auto_ml_ultra(df, target_col, time_col=None, min_score=0.88):\n",
    "#     t0 = time.time()\n",
    "#     print_step(\"Detecting problem type\")\n",
    "#     task = detect_problem_type(df, target_col, time_col)\n",
    "#     print(f\"[INFO] Task: {task}\")\n",
    "\n",
    "#     if task == \"time_series\":\n",
    "#         if _PROPHET_OK:\n",
    "#             print_step(\"Prophet Forecasting\")\n",
    "#             df2 = df[[time_col, target_col]].rename(columns={time_col: \"ds\", target_col: \"y\"})\n",
    "#             m = Prophet()\n",
    "#             m.fit(df2)\n",
    "#             future = m.make_future_dataframe(periods=10)\n",
    "#             forecast = m.predict(future)\n",
    "#             print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n",
    "#         if _PMDARIMA_OK:\n",
    "#             print_step(\"ARIMA Forecasting\")\n",
    "#             model = auto_arima(df[target_col], seasonal=False, trace=True)\n",
    "#             print(model.summary())\n",
    "#         print(\"[INFO] Time Series task finished.\")\n",
    "#         return\n",
    "\n",
    "#     print_step(\"Preprocessing\")\n",
    "#     X_proc, y, feat_names = preprocess_data(df, target_col, time_col)\n",
    "#     print(f\"[INFO] Features: {X_proc.shape[1]} | Samples: {X_proc.shape[0]}\")\n",
    "\n",
    "#     print_step(\"Splitting data\")\n",
    "#     if task == \"classification\":\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42, stratify=y)\n",
    "#         cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         score_metric = \"accuracy\"\n",
    "#     else:\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X_proc, y, test_size=0.2, random_state=42)\n",
    "#         cv = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         score_metric = \"r2\"\n",
    "#     print(f\"[INFO] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "#     print_step(\"Model selection & training\")\n",
    "#     models = get_models(task)\n",
    "#     results = []\n",
    "#     results_table = []\n",
    "#     for idx, (name, model) in enumerate(models):\n",
    "#         t1 = time.time()\n",
    "#         try:\n",
    "#             scores = cross_val_score(model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "#             score = scores.mean()\n",
    "#             std = scores.std()\n",
    "#             print(f\"[{idx+1:02d}/{len(models)}] {name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "#             results.append((score, name, model))\n",
    "#             results_table.append({\n",
    "#                 \"Model\": name,\n",
    "#                 \"CV_Mean\": score,\n",
    "#                 \"CV_Std\": std,\n",
    "#                 \"Time_sec\": time.time()-t1\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"[{idx+1:02d}/{len(models)}] {name}: ERROR {e}\")\n",
    "#             results_table.append({\n",
    "#                 \"Model\": name,\n",
    "#                 \"CV_Mean\": np.nan,\n",
    "#                 \"CV_Std\": np.nan,\n",
    "#                 \"Time_sec\": time.time()-t1,\n",
    "#                 \"Error\": str(e)\n",
    "#             })\n",
    "#     results.sort(reverse=True)\n",
    "#     top_models = results[:10]\n",
    "#     best_score, best_name, best_model = top_models[0]\n",
    "\n",
    "#     # Voting/Stacking\n",
    "#     print_step(\"Ensemble (Voting/Stacking)\")\n",
    "#     try:\n",
    "#         if task == \"classification\":\n",
    "#             voting = VotingClassifier(estimators=[(n, m) for _, n, m in top_models], voting='soft', n_jobs=-1)\n",
    "#             stacking = StackingClassifier(estimators=[(n, m) for _, n, m in top_models], final_estimator=LogisticRegression(max_iter=2000), n_jobs=-1)\n",
    "#         else:\n",
    "#             voting = VotingRegressor(estimators=[(n, m) for _, n, m in top_models], n_jobs=-1)\n",
    "#             stacking = StackingRegressor(estimators=[(n, m) for _, n, m in top_models], final_estimator=Ridge(), n_jobs=-1)\n",
    "#         for ens_name, ens_model in [(\"Voting\", voting), (\"Stacking\", stacking)]:\n",
    "#             t1 = time.time()\n",
    "#             scores = cross_val_score(ens_model, X_train, y_train, scoring=score_metric, cv=cv, n_jobs=-1)\n",
    "#             score = scores.mean()\n",
    "#             std = scores.std()\n",
    "#             print(f\"[Ensemble] {ens_name}: CV {score_metric} = {score:.4f} ± {std:.4f} | Time: {time.time()-t1:.1f}s\")\n",
    "#             results.append((score, ens_name, ens_model))\n",
    "#             results_table.append({\n",
    "#                 \"Model\": ens_name,\n",
    "#                 \"CV_Mean\": score,\n",
    "#                 \"CV_Std\": std,\n",
    "#                 \"Time_sec\": time.time()-t1\n",
    "#             })\n",
    "#             if score > best_score:\n",
    "#                 best_score, best_name, best_model = score, ens_name, ens_model\n",
    "#     except Exception as e:\n",
    "#         print(f\"Ensemble Error: {e}\")\n",
    "\n",
    "#     print_step(f\"Best Model: {best_name} (CV {score_metric}: {best_score:.4f})\")\n",
    "#     best_model.fit(X_train, y_train)\n",
    "#     y_pred = best_model.predict(X_test)\n",
    "#     if task == \"classification\":\n",
    "#         acc = accuracy_score(y_test, y_pred)\n",
    "#         print(f\"Test Accuracy: {acc:.4f}\")\n",
    "#         print(classification_report(y_test, y_pred))\n",
    "#         cm = confusion_matrix(y_test, y_pred)\n",
    "#         print(\"Confusion Matrix:\\n\", cm)\n",
    "#         if acc < min_score:\n",
    "#             print(f\"\\n[WARNING] Accuracy is less than {min_score*100:.1f}%!\")\n",
    "#     else:\n",
    "#         mse = mean_squared_error(y_test, y_pred)\n",
    "#         r2 = r2_score(y_test, y_pred)\n",
    "#         print(f\"Test R2: {r2:.4f}, MSE: {mse:.4f}\")\n",
    "#         if r2 < min_score:\n",
    "#             print(f\"\\n[WARNING] R2 is less than {min_score*100:.1f}%!\")\n",
    "\n",
    "#     if hasattr(best_model, \"feature_importances_\"):\n",
    "#         importances = best_model.feature_importances_\n",
    "#         idx = np.argsort(importances)[::-1][:10]\n",
    "#         print_step(\"Top 10 Features\")\n",
    "#         for i in idx:\n",
    "#             print(f\"{feat_names[i]}: {importances[i]:.4f}\")\n",
    "\n",
    "#     print(f\"\\n[INFO] Total pipeline time: {time.time()-t0:.1f} sec.\")\n",
    "\n",
    "#     print_step(\"Summary Table (All Models)\")\n",
    "#     df_results = pd.DataFrame(results_table)\n",
    "#     df_results = df_results.sort_values(\"CV_Mean\", ascending=False)\n",
    "#     display(df_results.reset_index(drop=True))\n",
    "\n",
    "\n",
    "# auto_ml_ultra(df, target_col, time_col=None)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d56b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pkg_resources\n",
    "# output_file = \"requirements.txt\"\n",
    "# packages = [\n",
    "#     \"numpy\",\n",
    "#     \"pandas\",\n",
    "#     \"numba\",\n",
    "#     \"scikit-learn\",\n",
    "#     \"matplotlib\",\n",
    "#     \"seaborn\"\n",
    "# ]\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     for package in packages:\n",
    "#         try:\n",
    "#             version = pkg_resources.get_distribution(package).version\n",
    "#             f.write(f\"{package}=={version}\\n\")\n",
    "#         except pkg_resources.DistributionNotFound:\n",
    "#             f.write(f\"{package}\\n\")  \n",
    "# print({output_file})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
